base_model: TheBloke/Llama-2-1b-chat-GGUF
model_type: LlamaForCausalLM
tokenizer_type: LlamaTokenizer

datasets:
  - path: faiss_index/{domain}/domain_train.json
    type: json

sequence_len: 1024
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05

learning_rate: 3e-4
train_batch_size: 8
num_epochs: 3